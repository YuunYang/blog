---
title:  "《机器学习》读书笔记"
categories:
  - machine learning
tags: 
  - 机器学习
  - 毕业设计
  - 周志华
entries_layout: grid
author_profile: true
toc: true
toc_label: "目录"
toc_sticky: true
---

周志华老师《机器学习》第一章绪论及第二章模型评估与选择

## 绪论

首先，了解机器学习是一门怎样的学科。

机器学习研究的主要内容，是关于在计算机上从数据中产生“模型”的算法，即“学习算法”，比如产生一个好瓜的模型。可以说机器学习是研究关于“学习算法”的学问。

### 基本术语
- 数据集（data set）：对所需研究的对象的数据的记录的集合；其中的每条记录是关于一件事物或对象的描述，称为一个“示例（instance）”或”样本（sample）“。
- 属性空间/样本空间/输入控件：属性张成的空间；如果把每一个属性作为坐标轴的一轴，则每一个示例都可以找到一个点与之对应；且由于每一个点都对应一个坐标向量，因此也把一个示例称为一个“特征向量（feature vector）”
- 训练数据（training data）：即训练过程中使用的数据，训练数据中的每一个样本称为”训练样本“，训练样本组成的集合称为”训练集“
- 分类（classification）：即欲预测的结果是离散的如预测西瓜到底是"好瓜"还是"坏瓜”
- 回归（regression）：预测的值为连续的值，如预测西瓜的成熟度是0.98还是0.37。可以分为“二分类”和“多分类”；预测任务是希望通过对训练集进行学习，建立一个从输入空间$$\chi$$到输出空间$$y$$的映射$$\text{f: }\chi]to\text{y}$$。与分类都属于“监督学习（supervised learning）”。
- 聚类（clustering）：即将训练集中的示例分成若干组，每组称为一个“簇（cluster）”；注意簇是自动形成的，其对应这一些潜在的概念。属于"无监督学习（unsupervised learning）"
- 泛化（generalization）：即簇划分能适用于没在训练集中出现的样本，学得模型适用与新样本的能力。
### 假设空间
首先介绍两个概念
- 归纳（induction）指从特殊到一般的泛化（generalization），即从具体的事实归结出一般性规律。
- 演绎（deduction）指从一般到特殊的特化（specialization），即从基础原理推演出具体状况。

对于归纳学习，最基本的是布尔概念学习，即是与不是。仅仅确定某一或几种特定的属性来表示是与否是远远不够的，因为我们在实际中往往会遇到之前没有遇到过的情况，也就是我们学习的目的是“泛化”；我们学习的过程就是在所有假设组成的空间中进行搜索的过程。
### 归纳偏好
归纳偏好就是指机器学习算法过程中对某种类型假设的偏好

为什么要有偏好？因为如果没有偏好，在每一次面临选择的时候，都进行随机选择，那么对于同一个模型，每次学习的结果都不一样，这样的学习是没有意义的。

“奥卡姆剃刀（occam’s razor）”是一种常用的，自然科学研究中最基本的用来引导算法确定正确性的原则。他的原则就是“若多个假设与观察一致选最简单的那个”。但哪一个是简单的却不是一个简单的问题。

另一个重要的概念是“没有免费的午餐定理（No Free Lunch Theorem）”；也就是在所有“问题”出现的机会相同或所有问题同等重要时，无论算法有多聪明，它们的期望始终是相同的。而实际情况是，我们关注的仅仅只是自己试图解决的问题，并不是关于这个问题的一个普世的答案，所以NFl定理的寓意即告诉我们，不能脱离具体问题，空泛讨论算法的合理性，因为如果考虑所有潜在的问题，则所有的学习算法都是一样的。我们应该关注学习算法的自身归纳偏好与问题是否相匹配。

## 模型评估与选择

### 经验误差与过拟合
首先理解“错误率（error rate）”和“误差（error）”。错误率：是指在机器学习分类过程中，将样本分类错误的数量占样本总数的比例；而误差则是更一般的概念，指实际预测输出与样本的真实输出之间的差异。

我们的目的是希望得到泛化误差小的学习器，很多时候，我们可以得到经验误差很小的学习器（即在给定的训练集的基础上，得到一个误差较小的学习器），但为了能在新样本上表现的好却是不容易的，当学习器把训练样本学得太好时，会把训练样本中自身的一些特点当做潜在样本的一般性质，造成“过拟合（overfitting）”，学习的不够会造成“欠拟合（underfitting）”

### 评估方法
- 留出法：直接将数据集D划分为两个互斥的集合，一个用作训练集，一个用作测试集。单次使用留出法会不太可靠，在使用留出法时，一般采用若干次随机划分、重复进行实验；如进行100次划分，每次产生一个训练/测试集用于评估结果，则100次后会得到一百次结果。
- 交叉验证法：先将数据集$$\text{D}$$划分为$$\text{k}$$个大小相似的互斥子集，即$$\text{D = }D_1\cup{D_2}\cup\ldots\cup{D_k}, D_i\cap{D_j}=\emptyset\text{i}\ne\text{j}$$；每次用$$\text{k-1}$$个子集的并集作为训练集，余下的那个子集作为测试集。讲数据集划分为$$\text{k}$$个子集，$$\text{k}$$折交叉验证通常随机使用不同的划分$$\text{p}$$次，最终的评估结果是这$$\text{p}$$次$$\text{k}$$折交叉验证结果的均值；例如“10次10折交叉验证”。若假定数据集中包含$$\text{m}$$个样本，若令上面的$$\text{k=m}$$，则称这种方式叫做“留一法（leave-one-out）”。

- 自助法：以自主采样法（bootstrap sampling）为基础。给定包含$$\text{m}$$个样本的数据集$$\text{D}$$，对它进行采集生成数据集$$\text{D'}$$：每次从$$\text{D}$$中挑选一个样本，将其拷贝到$$\text{D'}$$；这个过程重复m次后，即得到了包含m个样本的数据集$$\text{D'}$$。做一个简单的估计，样本在m次采样中始终不被采到的概率是$${(1-\frac{1}{m})}^m$$，取极限得到

$$\lim_{m\to \infty}{(1-\frac{1}{m})}^m\rightarrow\frac{1}{e}\approx\text{0.368}$$

### 性能度量
评估泛化性能，不仅需要有效可行的实验估计方法，还需要衡量泛化能力的评价标准，即“性能度量（performance measure）”。性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。要评估学习器$$\text{f}$$的性能，就要把学习器预测结果$$\text{f(x)}$$与真实标记$$\text{y}$$进行比较。

回归任务最常用的性能度量是“均方误差（mean squared error）”

$$E(f;D)= \frac{1}{m} \sum_{i=1}^m {(f(x_i)-y_i)}^2$$

更一般的，对于数据分布D和**概率密度函数p**，均方误差可描述成

$$E(f;D)= \int_{x\sim D} {(f(x)-y)}^2 p(x)$$

- 错误率和精度

错误率的定义为

$$E(f;D)= \frac{1}{m} \sum_{i=1}^m A(f(x_i)\ne y_i)$$

更一般的

$$E(f;D)= \int_{x\sim D} A{(f(x)\ne y)p(x)}$$

- 查准率、查全率与F1

仅仅使用错误率和精度并不能满足所有的任务需求，当需要对选出的样本中正例所占比例或者某个正例被分类成功的比例等类问题时，仅仅考虑有多少正例或反例时是不够的。

对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（true positive）、假正例、真反例（false negative）、真反例。（以西瓜书的例子为例，真正例指好瓜识别为好瓜的概率；假正例指坏瓜识别为好瓜的概率，其他的以此类推）。![table-01][table-01]{: .align-left}
查准率P与查全率R分别为：
$$\text{P=}\frac{TP}{TP+FP}$$
$$\text{R=}\frac{TP}{TP+FN}$$
$$TP+FP$$指的是所有识别为好瓜的个数，$$TP+FN$$指所有好瓜的个数。

查准率和查全率是一对矛盾的概念，一般来说是一高一低的存在，限于算法的水平和问题的大小，只有在一些简单的任务中才可能使两者都很高。

以查准率为纵轴，以查全率为横轴，就得到了查准率-查全率曲线，简称“P-R曲线”。![img-02][img-02]{: .align-right}

在进行比较时，若一个学习器的P-R曲线完全被另一个学习器的曲线“包住”，则可以判断后者优于前者，如上图可以判断A优于C。或者当两曲线相交时，可以根据曲线与坐标轴的之间的面积，可以在一定程度上判断孰优孰劣。但这个值很多时候都很难估计，所以还有另一些方法来度量。

平衡点（Break-Even Point），他是“查准率=查全率”时的取值，越大则可以认为越优，如图中可以认为学习器A优于B。

BEP过于简单，更常用的方法是F1度量：

$$F_1=\frac{2\times P\times R}{P+R} = \frac{2\times TP}{样例总数+TP-TN}$$

由于针对不同的应用，对查准率和查全率的重视程度也不一样，$$F_1$$度量的一般形式——$$F_\beta$$，当β=1时，退化为$$F_1$$，当β>1时查全率有更大的影响，当β<1时查准率有更大的影响。定义为：

$$F_\beta=\frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}$$

很多时候我们有多个二分类混淆矩阵，如进行多次训练/测试，每次得到一个混淆矩阵；或是在多个数据集上进行训练/测试，希望估计算法的“全局”性能；甚或是在执行多类任务，每两两类别的组合都对应一个混淆矩阵；……总之，我们希望在n个二分类混淆矩阵上综合考察查准率和查全率。

一种方法是分别计算出查准率和查全率，再计算平均值，这样得到“宏查准率（macro-P）”$$macro-P=\frac{1}{n}\sum_{i=1}^n P_i$$等；另一种是先将各种混淆矩阵对应的元素进行平均，得到TP等的平均值，记为$$\overline{TP}$$，再基于平均值求出“微查准率（micro-P）”$$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$

- ROC与AUC
- 代价敏感错误率与代价曲线

### 比较检验
针对实验评估方法和性能度量，下一步就需要对学习器的性能进行评估比较了，机器学习中的性能比较比较复杂，涉及多方面的因素
- 泛化性能，我们所得到的的性能度量是在测试ji
### 偏差与方差

[table-01]: /assets/images/2019-01-11-machine-learning-readingnotes/01.jpg
[img-02]: /assets/images/2019-01-11-machine-learning-readingnotes/02.png